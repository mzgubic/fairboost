{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from hep_ml.uboost import uBoostClassifier\n",
    "import sys\n",
    "sys.path.append('../fairboost')\n",
    "\n",
    "from generate import generate_toys\n",
    "from generate import show_variates\n",
    "from plot import show_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Show the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "show_variates(ax, generate_toys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normal bdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y, Z = generate_toys(10000)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, verbose=True)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "show_clf(clf, generate_toys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uniform boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Z = generate_toys(10000, pandas=True)\n",
    "\n",
    "clf = uBoostClassifier(n_estimators=100, uniform_features=['z'], uniform_label=1, train_features=['x1', 'x2'])\n",
    "clf.fit(X, Y)\n",
    "\n",
    "show_clf(clf, generate_toys, pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from scipy.special import logit\n",
    "from sklearn.ensemble._gb_losses import BinomialDeviance\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class FairboostClassifier:\n",
    "    \n",
    "    def __init__(self, Z, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        \n",
    "        self.Z = Z\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        #self.adversary = DecisionTreeRegressor()\n",
    "        \n",
    "        self._loss = BinomialDeviance(2)\n",
    "        self._estimators = []\n",
    "        self._gammas = []\n",
    "        \n",
    "    def _raw_prediction(self, X):\n",
    "        \n",
    "        # initial model TODO: this is loss function dependent\n",
    "        raw_prediction = logit(self.prior) * np.ones(len(X))\n",
    "        \n",
    "        # loop over estimators\n",
    "        for i, est in enumerate(self._estimators):\n",
    "            \n",
    "            # estimator response\n",
    "            pred = est.predict(X)\n",
    "            \n",
    "            # add them up\n",
    "            raw_prediction += self.learning_rate * pred\n",
    "        \n",
    "        return raw_prediction.reshape(-1,1)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        # force type\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        \n",
    "        # initial model\n",
    "        self.prior = np.sum(Y==1) / len(Y)\n",
    "        \n",
    "        # fit the remaining estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            \n",
    "            # predictions from the previous models\n",
    "            raw_prediction = self._raw_prediction(X)\n",
    "            raw_prediction_copy = raw_prediction.copy() # see sklearn documentation\n",
    "            \n",
    "            # compute the gradient\n",
    "            neg_grad = self._loss.negative_gradient(Y, raw_prediction_copy)\n",
    "            \n",
    "            # fit the new tree\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth,\n",
    "                                         criterion='friedman_mse')\n",
    "            tree.fit(X, neg_grad)\n",
    "            \n",
    "            # line search for each leaf (done in the loss function method)\n",
    "            sample_weight = np.ones(shape=Y.shape)\n",
    "            sample_mask = np.ones(shape=Y.shape, dtype=bool)\n",
    "            \n",
    "            self._loss.update_terminal_regions(\n",
    "                tree.tree_, X, Y, neg_grad, raw_prediction,\n",
    "                sample_weight, sample_mask, self.learning_rate)\n",
    "            \n",
    "            # append results\n",
    "            self._estimators.append(tree)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # make raw predictions\n",
    "        raw_prediction = self._raw_prediction(X)\n",
    "        \n",
    "        # turn them into a probability\n",
    "        proba = self._loss._raw_prediction_to_proba(raw_prediction)\n",
    "        \n",
    "        return proba\n",
    "\n",
    "def compare_classifiers(N):\n",
    "    \n",
    "    # training and test sets\n",
    "    X, Y, Z = generate_toys(N)\n",
    "    X_test, Y_test, Z_test = generate_toys(N)\n",
    "    \n",
    "    # fit the models\n",
    "    fb_clf = FairboostClassifier(Z, n_estimators=100)\n",
    "    sk_clf = GradientBoostingClassifier(n_estimators=100)\n",
    "    fb_clf.fit(X, Y)\n",
    "    sk_clf.fit(X, Y)\n",
    "    \n",
    "    # predict\n",
    "    fb_proba = fb_clf.predict_proba(X_test)[:,1]\n",
    "    sk_proba = sk_clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # evaluate\n",
    "    fb_score = roc_auc_score(Y_test, fb_proba)\n",
    "    sk_score = roc_auc_score(Y_test, sk_proba)\n",
    "    \n",
    "    print('Fairboost classifier:', fb_score)\n",
    "    show_clf(fb_clf, generate_toys)\n",
    "    print('Sklearn classifier:', sk_score)\n",
    "    show_clf(sk_clf, generate_toys)\n",
    "    \n",
    "compare_classifiers(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test sets\n",
    "X, Y, Z = generate_toys(10000)\n",
    "X_test, Y_test, Z_test = generate_toys(10000)\n",
    "\n",
    "# fit the models\n",
    "clf = FairboostClassifier(Z, n_estimators=100)\n",
    "clf.fit(X, Y)\n",
    "    \n",
    "# predict\n",
    "F_test = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as spo\n",
    "\n",
    "class PolynomialModel:\n",
    "    \"\"\"\n",
    "    Works only for one dimensional X...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, order=3):\n",
    "        \n",
    "        self.order = order\n",
    "        self.coefficients = np.ones(shape=self.order+1)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        # prepare objective\n",
    "        def loss(coefficients):\n",
    "            output = self._forward(X, *coefficients)\n",
    "            return np.mean((Y-output)**2)\n",
    "        \n",
    "        # fit parameters\n",
    "        res = spo.minimize(loss, x0=[1, 1, 1, 1])\n",
    "        \n",
    "        # save as coefficients\n",
    "        self.coefficients = res.x\n",
    "    \n",
    "    def _forward(self, X, *coefficients):\n",
    "        \n",
    "        # loop over coefficients\n",
    "        result = np.zeros(shape=X.shape)\n",
    "        for i, a in enumerate(coefficients):\n",
    "            result += a * X**i\n",
    "        return result\n",
    "    \n",
    "    def predict(self, X):\n",
    "        res = self._forward(X, *self.coefficients)\n",
    "        return res\n",
    "    \n",
    "    def negative_gradient(self, X):\n",
    "        pass\n",
    "\n",
    "# only take the signal values\n",
    "x = X_test[Y_test==1]\n",
    "f = F_test[Y_test==1]\n",
    "z = Z_test[Y_test==1]\n",
    "\n",
    "# create bins\n",
    "nbins=100\n",
    "hist, edges = np.histogram(f, range=(0,1), bins=nbins)\n",
    "centres = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "# compute metrics over bins\n",
    "idx = np.digitize(f, edges)\n",
    "bin_means = [np.mean(z[idx==i]) for i in range(1, len(edges))]\n",
    "bin_stds = [np.std(z[idx==i])/np.sqrt(len(z[idx==i])) for i in range(1, len(edges))]\n",
    "\n",
    "# fit the model\n",
    "adv = PolynomialModel()\n",
    "adv.fit(f, z)\n",
    "adv.predict(f)\n",
    "\n",
    "# create model response\n",
    "xs = np.linspace(0,1,100)\n",
    "ys = adv.predict(xs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(f, z, alpha=0.1, color='darkblue')\n",
    "ax.errorbar(centres, bin_means, xerr=0.5/nbins, yerr=bin_stds, color='orange')\n",
    "ax.plot(xs, ys, color='red')\n",
    "ax.set_xlabel('classifier output')\n",
    "ax.set_ylabel('Z')\n",
    "ax.set_xlim(0,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fairboo",
   "language": "python",
   "name": "fairboost"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
