{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from hep_ml.uboost import uBoostClassifier\n",
    "import sys\n",
    "sys.path.append('../fairboost')\n",
    "\n",
    "from generate import generate_toys\n",
    "from generate import show_variates\n",
    "from plot import show_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Show the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "show_variates(ax, generate_toys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normal bdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y, Z = generate_toys(10000)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, verbose=True)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "show_clf(clf, generate_toys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uniform boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X, Y, Z = generate_toys(100, pandas=True)\n",
    "\n",
    "clf = uBoostClassifier(n_estimators=100, uniform_features=['z'], uniform_label=1, train_features=['x1', 'x2'])\n",
    "clf.fit(X, Y)\n",
    "\n",
    "show_clf(clf, generate_toys, pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from scipy.special import logit\n",
    "from sklearn.ensemble._gb_losses import BinomialDeviance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from models import PolynomialModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def debug(x, f, z, Y, derivative, adv):\n",
    "    \n",
    "    # create model response\n",
    "    xs = np.linspace(0,1,100)\n",
    "    ys = adv.predict(xs)\n",
    "    \n",
    "    # create bins\n",
    "    nbins=100\n",
    "    hist, edges = np.histogram(f, range=(0,1), bins=nbins)\n",
    "    centres = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "    # compute metrics over bins\n",
    "    idx = np.digitize(f, edges)\n",
    "    bin_means = [np.mean(z[idx==i]) for i in range(1, len(edges))]\n",
    "    bin_stds = [np.std(z[idx==i])/np.sqrt(len(z[idx==i])) for i in range(1, len(edges))]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # show Z as a function of F\n",
    "    ax[0].scatter(f[Y==1], z[Y==1], alpha=0.1, color='darkblue')\n",
    "    ax[0].errorbar(centres, bin_means, xerr=0.5/nbins, yerr=bin_stds, color='lightblue')\n",
    "    ax[0].plot(xs, ys, color='red')\n",
    "    ax[0].set_xlabel('classifier output')\n",
    "    ax[0].set_ylabel('Z')\n",
    "    ax[0].set_xlim(0,1)\n",
    "    \n",
    "    # show variates\n",
    "    ax[1].scatter(X[Y==1,0], X[Y==1,1], alpha=0.5, c=derivative[Y==1], cmap='coolwarm')\n",
    "    ax[1].set_title('derivative')\n",
    "    ax[1].set_xlabel('x0')\n",
    "    ax[1].set_ylabel('x1')\n",
    "    ax[1].set_xlim(-1, 2)\n",
    "    ax[1].set_ylim(-1, 3)\n",
    "    plt.show()\n",
    "\n",
    "class FairboostClassifier:\n",
    "    \n",
    "    def __init__(self, Z, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        \n",
    "        self.Z = Z\n",
    "        self.lam = 1.0\n",
    "        self.order = 1\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self._loss = BinomialDeviance(2)\n",
    "        self._estimators = []\n",
    "        \n",
    "    def _raw_prediction(self, X):\n",
    "        \n",
    "        # initial model TODO: this is loss function dependent\n",
    "        raw_prediction = logit(self.prior) * np.ones(len(X))\n",
    "        \n",
    "        # loop over estimators\n",
    "        for i, est in enumerate(self._estimators):\n",
    "            \n",
    "            # estimator response\n",
    "            pred = est.predict(X)\n",
    "            \n",
    "            # add them up\n",
    "            raw_prediction += self.learning_rate * pred\n",
    "        \n",
    "        return raw_prediction.reshape(-1,1)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        # force type\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        \n",
    "        # initial model\n",
    "        self.prior = np.sum(Y==1) / len(Y)\n",
    "        \n",
    "        # fit the remaining estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            \n",
    "            # predictions from the previous models\n",
    "            raw_prediction = self._raw_prediction(X)\n",
    "            raw_prediction_copy = raw_prediction.copy() # see sklearn documentation\n",
    "            \n",
    "            # compute the gradient\n",
    "            neg_grad = self._loss.negative_gradient(Y, raw_prediction_copy)\n",
    "            \n",
    "            #########################\n",
    "            # the adversarial part\n",
    "            #########################\n",
    "            \n",
    "            # give it a bit of time\n",
    "            if i>9:\n",
    "                f = self.predict_proba(X)[:,1]\n",
    "                \n",
    "                dLdf = get_gradient_vanilla(f, self.Z, self.order, i, self.n_estimators)\n",
    "                \n",
    "                neg_grad += - self.lam * dLdf\n",
    "            \n",
    "            #########################\n",
    "            #########################\n",
    "            \n",
    "            # fit the new tree\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth,\n",
    "                                         criterion='friedman_mse')\n",
    "            tree.fit(X, neg_grad)\n",
    "            \n",
    "            # line search for each leaf (done in the loss function method)\n",
    "            sample_weight = np.ones(shape=Y.shape)\n",
    "            sample_mask = np.ones(shape=Y.shape, dtype=bool)\n",
    "            \n",
    "            # the following part makes implementation equivalent to sklearn but messes up the adversarial part\n",
    "            \n",
    "            #self._loss.update_terminal_regions(\n",
    "            #    tree.tree_, X, Y, neg_grad, raw_prediction,\n",
    "            #    sample_weight, sample_mask, self.learning_rate)\n",
    "            \n",
    "            # append results\n",
    "            self._estimators.append(tree)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # make raw predictions\n",
    "        raw_prediction = self._raw_prediction(X)\n",
    "        \n",
    "        # turn them into a probability\n",
    "        proba = self._loss._raw_prediction_to_proba(raw_prediction)\n",
    "        \n",
    "        return proba\n",
    "\n",
    "def get_gradient_vanilla(f, z, order, i, n_est):\n",
    "    \"\"\"\n",
    "    Mean square loss as the adversary loss.\n",
    "    Polynomial model of the Z as a function of f.\n",
    "    \"\"\"\n",
    "    # compute the adversary gradient: dL/df = dL/dA * dA/df\n",
    "    adv = PolynomialModel(order)\n",
    "    adv.fit(f, z)\n",
    "    A = adv.predict(f)\n",
    "                \n",
    "    dLdA = (A - z)\n",
    "    dAdf = adv.gradient(f)\n",
    "    dLdf = dLdA * dAdf # dL/df. in my understanding should have a negative sign in front but then it doesnt work empirically\n",
    "                \n",
    "    if i%10==0:\n",
    "        print('{}/{}'.format(i, n_est))\n",
    "        debug(X, f, z, Y, dLdf, adv)\n",
    "            \n",
    "    # combine to the total gradient\n",
    "    return dLdf\n",
    "    \n",
    "def compare_classifiers(N, generate):\n",
    "\n",
    "    # training and test sets\n",
    "    X, Y, Z = generate(N)\n",
    "    X_test, Y_test, Z_test = generate(N)\n",
    "\n",
    "    # fit the models\n",
    "    fb_clf = FairboostClassifier(Z, n_estimators=100)\n",
    "    sk_clf = GradientBoostingClassifier(n_estimators=100)\n",
    "    fb_clf.fit(X, Y)\n",
    "    sk_clf.fit(X, Y)\n",
    "\n",
    "    # predict\n",
    "    fb_proba = fb_clf.predict_proba(X_test)[:,1]\n",
    "    sk_proba = sk_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # evaluate\n",
    "    fb_score = roc_auc_score(Y_test, fb_proba)\n",
    "    sk_score = roc_auc_score(Y_test, sk_proba)\n",
    "\n",
    "    print('Fairboost classifier:', fb_score)\n",
    "    show_clf(fb_clf, generate)\n",
    "    print('Sklearn classifier:', sk_score)\n",
    "    show_clf(sk_clf, generate)\n",
    "\n",
    "compare_classifiers(10000, generate_toys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "show_variates(ax, generate_toys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fairboo",
   "language": "python",
   "name": "fairboost"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
