{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from hep_ml.uboost import uBoostClassifier\n",
    "import sys\n",
    "sys.path.append('../fairboost')\n",
    "\n",
    "from generate import generate_toys\n",
    "from generate import show_variates\n",
    "from plot import show_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Show the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "show_variates(ax, generate_toys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normal bdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y, Z = generate_toys(10000)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, verbose=True)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "show_clf(clf, generate_toys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uniform boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Z = generate_toys(10000, pandas=True)\n",
    "\n",
    "clf = uBoostClassifier(n_estimators=100, uniform_features=['z'], uniform_label=1, train_features=['x1', 'x2'])\n",
    "clf.fit(X, Y)\n",
    "\n",
    "show_clf(clf, generate_toys, pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from scipy.special import logit\n",
    "from sklearn.ensemble._gb_losses import BinomialDeviance\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class FairboostClassifier:\n",
    "    \n",
    "    def __init__(self, Z, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        \n",
    "        self.Z = Z\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        #self.adversary = DecisionTreeRegressor()\n",
    "        \n",
    "        self._loss = BinomialDeviance(2)\n",
    "        self._estimators = []\n",
    "        self._gammas = []\n",
    "        \n",
    "    def _raw_prediction(self, X):\n",
    "        \n",
    "        # initial model TODO: this is loss function dependent\n",
    "        raw_prediction = logit(self.prior) * np.ones(len(X))\n",
    "        \n",
    "        # loop over estimators\n",
    "        for i, est in enumerate(self._estimators):\n",
    "            \n",
    "            # estimator response\n",
    "            pred = est.predict(X)\n",
    "            \n",
    "            # add them up\n",
    "            raw_prediction += self.learning_rate * pred\n",
    "        \n",
    "        return raw_prediction.reshape(-1,1)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        # force type\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        \n",
    "        # initial model\n",
    "        self.prior = np.sum(Y==1) / len(Y)\n",
    "        \n",
    "        # fit the remaining estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            \n",
    "            # predictions from the previous models\n",
    "            raw_prediction = self._raw_prediction(X)\n",
    "            raw_prediction_copy = raw_prediction.copy() # see sklearn documentation\n",
    "            \n",
    "            # compute the gradient\n",
    "            neg_grad = self._loss.negative_gradient(Y, raw_prediction_copy)\n",
    "            \n",
    "            # fit the new tree\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth,\n",
    "                                         criterion='friedman_mse')\n",
    "            tree.fit(X, neg_grad)\n",
    "            \n",
    "            # line search for each leaf (done in the loss function method)\n",
    "            sample_weight = np.ones(shape=Y.shape)\n",
    "            sample_mask = np.ones(shape=Y.shape, dtype=bool)\n",
    "            \n",
    "            self._loss.update_terminal_regions(\n",
    "                tree.tree_, X, Y, neg_grad, raw_prediction,\n",
    "                sample_weight, sample_mask, self.learning_rate)\n",
    "            \n",
    "            # append results\n",
    "            self._estimators.append(tree)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # make raw predictions\n",
    "        raw_prediction = self._raw_prediction(X)\n",
    "        \n",
    "        # turn them into a probability\n",
    "        proba = self._loss._raw_prediction_to_proba(raw_prediction)\n",
    "        \n",
    "        return proba\n",
    "\n",
    "def compare_classifiers(N):\n",
    "    \n",
    "    # training and test sets\n",
    "    X, Y, Z = generate_toys(N)\n",
    "    X_test, Y_test, Z_test = generate_toys(N)\n",
    "    \n",
    "    # fit the models\n",
    "    fb_clf = FairboostClassifier(Z, n_estimators=100)\n",
    "    sk_clf = GradientBoostingClassifier(n_estimators=100)\n",
    "    fb_clf.fit(X, Y)\n",
    "    sk_clf.fit(X, Y)\n",
    "    \n",
    "    # predict\n",
    "    fb_proba = fb_clf.predict_proba(X_test)[:,1]\n",
    "    sk_proba = sk_clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # evaluate\n",
    "    fb_score = roc_auc_score(Y_test, fb_proba)\n",
    "    sk_score = roc_auc_score(Y_test, sk_proba)\n",
    "    \n",
    "    print('Fairboost classifier:', fb_score)\n",
    "    show_clf(fb_clf, generate_toys)\n",
    "    print('Sklearn classifier:', sk_score)\n",
    "    show_clf(sk_clf, generate_toys)\n",
    "    \n",
    "compare_classifiers(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fairboo",
   "language": "python",
   "name": "fairboost"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
